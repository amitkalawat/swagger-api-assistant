{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Generative AI Media Entertainment Workshop\n",
    "## Prompt Engineering with Amazon Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`Python 3`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the [`boto3` Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to work with [Amazon Bedrock](https://aws.amazon.com/bedrock/) Foundation Models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeedd9f-f0a3-4f8e-934d-22f6f7a89de5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Run the cells in this section to install the packages needed by the notebooks in this workshop. ⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108c611c-7246-45c4-9f1e-76888b5076eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3>=1.28.57\n",
      "  Downloading boto3-1.34.118-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Downloading awscli-1.33.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Downloading botocore-1.34.118-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.57)\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting docutils<0.17,>=0.10 (from awscli>=1.29.57)\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.7,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading boto3-1.34.118-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading awscli-1.33.0-py3-none-any.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.118-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.6.0\n",
      "    Uninstalling pyasn1-0.6.0:\n",
      "      Successfully uninstalled pyasn1-0.6.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0\n",
      "    Uninstalling python-dateutil-2.9.0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.51\n",
      "    Uninstalling botocore-1.34.51:\n",
      "      Successfully uninstalled botocore-1.34.51\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.1\n",
      "    Uninstalling s3transfer-0.10.1:\n",
      "      Successfully uninstalled s3transfer-0.10.1\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.34.51\n",
      "    Uninstalling boto3-1.34.51:\n",
      "      Successfully uninstalled boto3-1.34.51\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-ai 2.15.0 requires faiss-cpu, which is not installed.\n",
      "aiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.34.118 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.33.0 boto3-1.34.118 botocore-1.34.118 colorama-0.4.6 docutils-0.16 jmespath-1.0.1 pyasn1-0.6.0 python-dateutil-2.9.0.post0 rsa-4.7.2 s3transfer-0.10.1 six-1.16.0 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the boto3 client\n",
    "\n",
    "Interaction with the Bedrock API is done via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "#### Use different clients\n",
    "The boto3 provides different clients for Amazon Bedrock to perform different actions. The actions for [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) are supported by Amazon Bedrock Runtime where as other operations, such as [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html), are handled via [Amazon Bedrock client](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html).\n",
    "\n",
    "\n",
    "#### Use the default credential chain\n",
    "\n",
    "If you are running this notebook from [Amazon Sagemaker Studio](https://aws.amazon.com/sagemaker/studio/) and your Sagemaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9174c4-326a-463e-92e1-8c7e47111269",
   "metadata": {},
   "source": [
    "#### Validate the connection\n",
    "\n",
    "We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67b4466-12ff-4975-9811-7a19c6206604",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '62c74c94-bd70-4613-9906-bfc4a58a80f0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 04 Jun 2024 00:54:49 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '23453',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '62c74c94-bd70-4613-9906-bfc4a58a80f0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1',\n",
       "   'modelId': 'amazon.titan-image-generator-v1',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct',\n",
       "   'modelName': 'J2 Grande Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct',\n",
       "   'modelName': 'J2 Jumbo Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid',\n",
       "   'modelId': 'ai21.j2-mid',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid-v1',\n",
       "   'modelId': 'ai21.j2-mid-v1',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra',\n",
       "   'modelId': 'ai21.j2-ultra',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra-v1:0:8k',\n",
       "   'modelId': 'ai21.j2-ultra-v1:0:8k',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra-v1',\n",
       "   'modelId': 'ai21.j2-ultra-v1',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "   'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:18k',\n",
       "   'modelId': 'anthropic.claude-v2:0:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:100k',\n",
       "   'modelId': 'anthropic.claude-v2:0:100k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-text-v14:7:4k',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-r-v1:0',\n",
       "   'modelId': 'cohere.command-r-v1:0',\n",
       "   'modelName': 'Command R',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-r-plus-v1:0',\n",
       "   'modelId': 'cohere.command-r-plus-v1:0',\n",
       "   'modelName': 'Command R+',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3:0:512',\n",
       "   'modelId': 'cohere.embed-english-v3:0:512',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "   'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelName': 'Mistral 7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelName': 'Mixtral 8x7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "   'modelName': 'Mistral Large',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3_bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 1 - Text Summarization in Bedrock\n",
    "\n",
    "Picture yourself at a movie studio, or maybe a video streaming company. You get synopses all the time, and hardly have time to read them. In this Lab you will use Foundation Models (FMs) in Amazon Bedrock to summarize synopsis, so it is much easier to read and digest.\n",
    "\n",
    "Here is a [Synopsis for the movie Whiplash](https://www.scriptreaderpro.com/wp-content/uploads/2019/07/Film-Synopsis-Example-Whiplash.pdf). We will use it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0a79b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json \n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893872fe-04fa-4f09-9736-6c6173ec1fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cohere Command\n",
    "\n",
    "Let's create a prompt and ask the model to summarize some text for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df55eed-a3cf-426c-95ea-ec60dade6477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "Andrew Neiman, a young jazz student at the Shaffer Conservatory in New York, has one dream:\n",
    "to go down in history as one of the world’s best drummers. He’s therefore thrilled when Terence\n",
    "Fletcher, a famous conductor, invites him to join the conservatory’s Studio Band as a core\n",
    "alternate drummer. Fletcher, however, turns out to be anything but an ordinary teacher. He’s a\n",
    "sadistic tyrant and Andrew realizes just how much of one when he has a chair hurled at him for\n",
    "failing to keep time.\n",
    "At a jazz competition, Andrew misplaces the sheet music to “Whiplash,” meaning their core\n",
    "drummer can’t play. Andrew, however, can—from memory—and after a first class performance,\n",
    "Fletcher promotes him to core drummer. But Andrew’s joy won’t last long… In a typically\n",
    "twisted move, Fletcher bumps Andrew back down to alternate drummer, putting a much lesstalented \n",
    "musician in his place. More determined than ever, Andrew breaks up with his girlfriend\n",
    "and practices until his hands bleed. It pays off… After a grueling five-hour audition, during\n",
    "which Fletcher kicks furniture and screams at him, Andrew earns back the core spot.\n",
    "Andrew arrives late for another competition after his bus breaks down, hires a car, then realizes\n",
    "he left his drumsticks at the car rental office. He races back, retrieves them, but on his way to\n",
    "the theater, his car is broadsided by a semi. He crawls from the wreckage and runs the rest of\n",
    "the way, finally arriving on stage bloody and injured. When he struggles to play, Fletcher cooly\n",
    "dismisses him. Enraged, Andrew attacks Fletcher in front of the audience, which gets him\n",
    "dismissed from the school.\n",
    "Andrew files an ethics complaint against Shaffer Conservatory and learns that one of Fletcher’s\n",
    "former students hanged himself due to his emotional and physical abuse. Andrew agrees to\n",
    "testify as an anonymous witness and Fletcher is fired. Andrew gives up drumming and, months\n",
    "later, stumbles upon Fletcher playing piano in a jazz club. They go for a drink, during which\n",
    "Fletcher explains why he pushed his students so hard: so that they might become the next\n",
    "Charlie Parker. In Fletcher’s eyes the greats like Parker wouldn’t be discouraged by anything.\n",
    "He then invites Andrew to drum with his band at a jazz festival. Has Fletcher changed? Andrew\n",
    "thinks so, and accepts.\n",
    "On stage at the festival, Fletcher has two surprises for Andrew. One: he knows he testified\n",
    "against him, and two: they’re starting with a piece Andrew doesn’t know and for which there’s\n",
    "no sheet music. Unable to play, Andrew leaves the stage humiliated. But he returns, interrupts\n",
    "Fletcher and cues the band, before launching into a breathtaking solo. Fletcher is taken aback,\n",
    "but in that moment realizes the enormity of Andrew’s talent and begins to guide him. As\n",
    "Andrew ends his solo, they share a smile and Fletcher cues the finale.\n",
    "\n",
    "{INPUT}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a8eb1-f9a4-4946-bfe2-550d21487f48",
   "metadata": {},
   "source": [
    "Next, we will construct the body and response to pass the prompt above to the Cohere Command text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2bb671-6b10-4948-9e5e-95d6ced3b86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mYou don't have access to the model with the specified model ID.                \n",
      "To troubeshoot this issue please refer to the following resources.                 \n",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html                 \n",
      "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    input_text = prompt_data.replace(\"{INPUT}\", \"Summarize the text above:\")\n",
    "    body = json.dumps({\n",
    "      \"prompt\": input_text,\n",
    "      \"max_tokens\":400,\n",
    "      \"temperature\":0.75 \n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"cohere.command-text-v14\", \n",
    "      accept= \"*/*\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    parse_text = response_body['generations'][0]['text']\n",
    "\n",
    "    print(parse_text)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae706235",
   "metadata": {},
   "source": [
    "## Refine the output\n",
    "\n",
    "This is good, but there are ways to refine this result.\n",
    "\n",
    "## Prompt engineering\n",
    "Prompt engineering is a discipline focused on developing optimized prompts to efficiently apply language models to various tasks.\n",
    "\n",
    "Try the same synopsis, but this time followed by \"Summarize the text above in one sentence:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc0cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mYou don't have access to the model with the specified model ID.                \n",
      "To troubeshoot this issue please refer to the following resources.                 \n",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html                 \n",
      "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_text = prompt_data.replace(\"{INPUT}\", \"Summarize the text above in one sentence:\")\n",
    "    \n",
    "    body = json.dumps({\n",
    "      \"prompt\": input_text,\n",
    "      \"max_tokens\":400,\n",
    "      \"temperature\":0.75 \n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"cohere.command-text-v14\", \n",
    "      accept= \"*/*\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    parse_text = response_body['generations'][0]['text']\n",
    "    parse_text\n",
    "\n",
    "    print(parse_text)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45fbfd",
   "metadata": {},
   "source": [
    "## Your turn \n",
    "\n",
    "Modify the prompt below to output the summary into one paragraph with output similar to the one below:\n",
    "\n",
    "```\n",
    "Andrew Neiman is a young drummer who dreams of greatness. He joins the Shaffer Conservatory in New York and is invited to join a band led by the conductor Terence Fletcher. However, Fletcher turns out to be a tyrannical and sadistic bandleader who abuses his students. Despite this, Andrew is determined to succeed and works hard to impress Fletcher. He is eventually promoted to core drummer but is later demoted. This motivates Andrew to practice even harder, and he eventually earns back his core spot. However, Fletcher continues to abuse him, and Andrew eventually testifies against him, leading to Fletcher's dismissal from the school. Months later, Andrew encounters Fletcher again and decides to give him a chance, but Fletcher retaliates by setting up a situation where Andrew fails again. However, Andrew manages to turn the situation around and impresses Fletcher in the end.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8749f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Parameter cannot be an empty string...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     USER_INPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# INPUT YOUR ANSWER HERE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m USER_INPUT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter cannot be an empty string...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m prompt_data\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{INPUT}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, USER_INPUT)\n\u001b[1;32m      9\u001b[0m     body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[1;32m     10\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_text,\n\u001b[1;32m     11\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m     12\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.75\u001b[39m \n\u001b[1;32m     13\u001b[0m     })\n",
      "\u001b[0;31mAssertionError\u001b[0m: Parameter cannot be an empty string..."
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    USER_INPUT = \"\" # INPUT YOUR ANSWER HERE\n",
    "    assert USER_INPUT, \"Parameter cannot be an empty string...\"\n",
    "\n",
    "    input_text = prompt_data.replace(\"{INPUT}\", USER_INPUT)\n",
    "\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"prompt\": input_text,\n",
    "      \"max_tokens\":400,\n",
    "      \"temperature\":0.75 \n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"cohere.command-text-v14\", \n",
    "      accept= \"*/*\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    parse_text = response_body['generations'][0]['text']\n",
    "    parse_text\n",
    "\n",
    "    print(parse_text)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f659f9",
   "metadata": {},
   "source": [
    "## Changing prompt parameters.\n",
    "Lets see what happens if we change the prompt parameters.\n",
    "\n",
    "Below we are going to change the max_tokens to 50 lets see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"prompt\": input_text,\n",
    "      \"max_tokens\":50,\n",
    "      \"temperature\":0.2 \n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"cohere.command-text-v14\", \n",
    "      accept= \"*/*\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    parse_text = response_body['generations'][0]['text']\n",
    "    parse_text\n",
    "\n",
    "    print(parse_text)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c440f",
   "metadata": {},
   "source": [
    "----\n",
    "# Lab 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8399c1c",
   "metadata": {},
   "source": [
    "# Text Generation in Bedrock\n",
    "If you are working with a script (whether for movies, television, game, etc), Foundation Models (FMs) can assist in a number of ways.\n",
    "\n",
    "## Use FM to create dialogue\n",
    "**FMs can generate dialogues for you.** - give the model a list of characters and a brief description of the scene, and let FMs generate the dialogues for a Comedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "Human: as an expert script writer, write the dialogue between a husband and wife for a comedy scene.\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "try:\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"prompt\": prompt_data,\n",
    "      \"max_tokens_to_sample\":256,\n",
    "      \"top_k\":250,\n",
    "      \"stop_sequences\":[], #define phrases that signal the model to conclude text generation.\n",
    "      \"temperature\":0.5, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "      \"top_p\":1 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "      modelId=\"anthropic.claude-v2\", \n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "\n",
    "    def llm_output_parser(*args, width: int = 100, **kwargs):\n",
    "        buffer = StringIO()\n",
    "        try:\n",
    "            # Redirect sys.stdout to capture the output\n",
    "            _stdout = sys.stdout\n",
    "            sys.stdout = buffer\n",
    "            print(*args, **kwargs)\n",
    "            output = buffer.getvalue()\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that may occur during capturing\n",
    "            print(f\"Error capturing output: {e}\")\n",
    "            return\n",
    "        finally:\n",
    "            # Restore the original sys.stdout\n",
    "            sys.stdout = _stdout\n",
    "    \n",
    "        try:\n",
    "            # Wrap lines and print the parsed output\n",
    "            for line in output.splitlines():\n",
    "                print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that may occur during line wrapping\n",
    "            print(f\"Error wrapping lines: {e}\")\n",
    "            \n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    llm_output_parser(response_body.get('completion'))\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07843b96",
   "metadata": {},
   "source": [
    "## FMs can also help you brainstorm for plot ideas.\n",
    "\n",
    "Ask Claude for generate some plot ideas using the prompt below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "Human: as an award wining director, give me some ideas about a plot-point in a romantic comedy involving two young professionals who just met by chance in the supermarket after loosing track of one another after college.\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cef1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "try:\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"prompt\": prompt_data,\n",
    "      \"max_tokens_to_sample\":256,\n",
    "      \"top_k\":250,\n",
    "      \"stop_sequences\":[], #define phrases that signal the model to conclude text generation.\n",
    "      \"temperature\":0.5, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "      \"top_p\":1 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "      modelId=\"anthropic.claude-v2\", \n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "\n",
    "    def llm_output_parser(*args, width: int = 100, **kwargs):\n",
    "        buffer = StringIO()\n",
    "        try:\n",
    "            # Redirect sys.stdout to capture the output\n",
    "            _stdout = sys.stdout\n",
    "            sys.stdout = buffer\n",
    "            print(*args, **kwargs)\n",
    "            output = buffer.getvalue()\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that may occur during capturing\n",
    "            print(f\"Error capturing output: {e}\")\n",
    "            return\n",
    "        finally:\n",
    "            # Restore the original sys.stdout\n",
    "            sys.stdout = _stdout\n",
    "    \n",
    "        try:\n",
    "            # Wrap lines and print the parsed output\n",
    "            for line in output.splitlines():\n",
    "                print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that may occur during line wrapping\n",
    "            print(f\"Error wrapping lines: {e}\")\n",
    "            \n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    llm_output_parser(response_body.get('completion'))\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a99090",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab 3\n",
    "# Image Generation in Bedrock\n",
    "\n",
    "Now that you have text generation , let's try image generation with Titan Image Generator  on Bedrock.\n",
    "\n",
    ">Before we get started lets restart the kernel. \n",
    "\n",
    "### Image Prompting\n",
    "\n",
    "Writing a good prompt can be somewhat of an art.\n",
    "\n",
    "It is often difficult to predict whether a given prompt will yield a satisfactory result with a certain model.\n",
    "\n",
    "However, there are certain templates that have been known to work.\n",
    "\n",
    "Broadly, a prompt can be broken down into three pieces:\n",
    "- Type of image (photograph/sketch/painting)\n",
    "- Description of the content (subject/object/environment/scene/&c.), and\n",
    "- Style of the image (realistic/artistic).\n",
    "\n",
    "You can change each of the three parts individually to generate variations of an image.\n",
    "Adjectives have been known to play a significant role in the image generation process.\n",
    "Also, adding more details help in the generation process.\n",
    "\n",
    "In order to generate a realistic image, you can use phrases such as:\n",
    "- a photo of\n",
    "- a photograph of\n",
    "- realistic\n",
    "- hyper realistic\n",
    "\n",
    "To generate something more artistic, you can use phrases like:\n",
    "- by Pablo Picasso\n",
    "- oil painting by Rembrandt\n",
    "- landscape art by Frederic Edwin Church\n",
    "- pencil drawing by Albrecht Dürer\n",
    "\n",
    "You can also combine different artists as well.\n",
    "To generate artistic images by category, you can add the art category in the prompt such as\n",
    "lion on a beach, abstract\n",
    "\n",
    "Some other categories include:\n",
    "- oil painting\n",
    "- pencil drawing\n",
    "- pop art\n",
    "- digital art\n",
    "- anime\n",
    "- cartoon\n",
    "- futurism\n",
    "- watercolor\n",
    "- manga\n",
    "\n",
    "\n",
    "You can also include details such as lighting or camera lens such as:\n",
    "- 35mm wide lens\n",
    "- 85mm wide lens\n",
    "\n",
    "or details about the framing:\n",
    "- portrait\n",
    "- landscape\n",
    "- close up\n",
    "\n",
    "Note that models can generate different images even if same prompt is given multiple times.\n",
    "\n",
    "So, you can generate multiple images and select the image that suits your application best.\n",
    "\n",
    "For more information on Amazon Titan Image Generator prompt engineering, see [Amazon Titan Image Generator Prompt Engineering Best Practices.](https://d2eo22ngex1n9g.cloudfront.net/Documentation/User+Guides/Titan/Amazon+Titan+Image+Generator+Prompt+Engineering+Guidelines.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# External dependencies\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import botocore\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfa9e6",
   "metadata": {},
   "source": [
    "\n",
    "## Text to Image\n",
    "\n",
    "In text-to-image mode, we provide a text description (prompt) of the image that should be generated.\n",
    "\n",
    "What if we want to avoid specific content or stylistic choices? Because image generation models are typically trained from image descriptions, trying to directly specify what you don't want in the prompt (e.g. man without a beard) doesn't usually work well: it would be very unusual to describe an image by what it is not!\n",
    "\n",
    "In the case of Amazon Titan Image Generator, we can specify a negative prompt to steer the model away from unwanted elements\n",
    "\n",
    "For our attempt we will use \"a photograph of an astronaut riding a horse\" and \"nsfw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "negative_prompts = \"nsfw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1d1a2",
   "metadata": {},
   "source": [
    "The Amazon Bedrock InvokeModel provides access to Amazon Titan Image Generator by setting the right model ID, and returns a JSON response including a Base64 encoded string that represents the (PNG) image.\n",
    "\n",
    "When making an InvokeModel request, we need to fill the body field with a JSON object that varies depending on the task (taskType) you wish to perform viz. text to image, image variation, inpainting or outpainting. The Amazon Titan models supports the following parameters:\n",
    "\n",
    "- cfgscale - determines how much the final image reflects the prompt\n",
    "- seed - a number used to initialize the generation, using the same seed with the same prompt + settings combination will produce the same results\n",
    "- numberOfImages - the number of times the image is sampled and produced\n",
    "- quality - determines the output image quality (standard or premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payload\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"taskType\": \"TEXT_IMAGE\",\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt,                    # Required\n",
    "            \"negativeText\": negative_prompts   # Optional\n",
    "        },\n",
    "        \"imageGenerationConfig\": {\n",
    "            \"numberOfImages\": 1,   # Range: 1 to 5 \n",
    "            \"quality\": \"standard\",  # Options: standard or premium\n",
    "            \"height\": 1024,        # Supported height list in the docs \n",
    "            \"width\": 1024,         # Supported width list in the docs\n",
    "            \"cfgScale\": 8,       # Range: 1.0 (exclusive) to 10.0\n",
    "            \"seed\": 1             # Range: 0 to 214783647\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Make model request\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=\"amazon.titan-image-generator-v1\",\n",
    "    accept=\"application/json\", \n",
    "    contentType=\"application/json\"\n",
    ")\n",
    "\n",
    "# Process the image\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "img1_b64 = response_body[\"images\"][0]\n",
    "\n",
    "# Debug\n",
    "print(f\"Output: {img1_b64[0:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac360ac",
   "metadata": {},
   "source": [
    "By decoding our image string and loading it with an image processing library like [Pillow](https://pillow.readthedocs.io/en/stable/), we can display and manipulate the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open(\n",
    "    io.BytesIO(\n",
    "        base64.decodebytes(\n",
    "            bytes(img1_b64, \"utf-8\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#print(os.getcwd())\n",
    "img1.save(\"image_1.png\")\n",
    "\n",
    "# Display\n",
    "img1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdd502",
   "metadata": {},
   "source": [
    "## Modify an Image\n",
    "\n",
    "You can modify the image you just generated, this will further constrain the image generated. Let's change the prompt to **_Photograph of a astronaut riding a llama_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e236363",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a llama\"\n",
    "negative_prompts = \"nsfw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd1caa",
   "metadata": {},
   "source": [
    "We will read the image file to a base64 object to pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(img) -> str:\n",
    "    \"\"\"Converts a PIL Image or local image file path to a base64 string\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")\n",
    "\n",
    "img1_b64 = image_to_base64(img1)\n",
    "print(f\"Input: {img1_b64[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b93ae",
   "metadata": {},
   "source": [
    "Lets pass this image to the model with our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e63f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload creation\n",
    "body = json.dumps({\n",
    "     \"taskType\": \"IMAGE_VARIATION\",\n",
    "     \"imageVariationParams\": {\n",
    "         \"text\": prompt,              # Optional\n",
    "         \"negativeText\": negative_prompts,   # Optional\n",
    "         \"images\": [img1_b64],               # One image is required\n",
    "     },\n",
    "     \"imageGenerationConfig\": {\n",
    "         \"numberOfImages\": 1,\n",
    "         \"quality\": \"premium\",\n",
    "         \"height\": 1024,\n",
    "         \"width\": 1024,\n",
    "         \"cfgScale\": 8,\n",
    "         \"seed\": 1\n",
    "     }\n",
    " })\n",
    "\n",
    "# Model invocation\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=\"amazon.titan-image-generator-v1\",\n",
    "    accept=\"application/json\", \n",
    "    contentType=\"application/json\"\n",
    ")\n",
    "\n",
    "# Output processing\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "img2_b64 = response_body[\"images\"][0]\n",
    "\n",
    "# Debug\n",
    "print(f\"Output: {img2_b64[0:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96654cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = Image.open(\n",
    "    io.BytesIO(\n",
    "        base64.decodebytes(\n",
    "            bytes(img2_b64, \"utf-8\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#print(os.getcwd())\n",
    "img2.save(\"image_2.png\")\n",
    "\n",
    "# Display\n",
    "img2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8fc372",
   "metadata": {},
   "source": [
    "### Inpainting\n",
    "\n",
    "Another way to modify images is by using inpainting.\n",
    "\n",
    "Inpainting refers to the process of replacing a portion of an image with another image based on a textual prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980b11c",
   "metadata": {},
   "source": [
    "Let's define what we want to change in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a11e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a camel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00566f",
   "metadata": {},
   "source": [
    "Next we will pass the previous image to the model with the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388116b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload creation\n",
    "body = json.dumps({\n",
    "    \"taskType\": \"INPAINTING\",\n",
    "    \"inPaintingParams\": {\n",
    "        \"text\": prompt,              # Optional\n",
    "        \"negativeText\": negative_prompts,    # Optional\n",
    "        \"image\": img2_b64,      # Required\n",
    "        \"maskPrompt\": \"llama\",               # One of \"maskImage\" or \"maskPrompt\" is required\n",
    "        # \"maskImage\": image_to_base64(mask),  # Input maskImage based on the values 0 (black) or 255 (white) only\n",
    "    },                                                 \n",
    "    \"imageGenerationConfig\": {\n",
    "        \"numberOfImages\": 1,\n",
    "        \"quality\": \"premium\",\n",
    "        \"height\": 1024,\n",
    "        \"width\": 1024,\n",
    "        \"cfgScale\": 8,\n",
    "        \"seed\": 0\n",
    "    }\n",
    "})\n",
    "\n",
    "# Model invocation\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=\"amazon.titan-image-generator-v1\",\n",
    "    accept=\"application/json\", \n",
    "    contentType=\"application/json\"\n",
    ")\n",
    "\n",
    "# Output processing\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "img3_b64 = response_body[\"images\"][0]\n",
    "print(f\"Output: {img3_b64[0:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263629be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpaint = Image.open(\n",
    "    io.BytesIO(\n",
    "        base64.decodebytes(\n",
    "            bytes(img3_b64, \"utf-8\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "inpaint.save(\"image_3.png\")\n",
    "inpaint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68829ce8",
   "metadata": {},
   "source": [
    "----\n",
    "# Lab 4\n",
    "# Question Answering in Bedrock\n",
    "\n",
    "Question answering (QA) is an important task that involves extracting answers to factual queries posed in natural language. Foundation Models (FMs) like Amazon Titan, Anthropic Claude, and Cohere Command are trained on vast amounts of text. From that training, they develop the probability distributions that can predict next token/word in an answer sequence when given a question.\n",
    "\n",
    "Despite how good these models have become, they are prone to **hallucination**, a phenomenon where a FMs generates false or inaccurate information. In the lab, we will experiment with different techniques to reduce hallucination and improve model response accuracy.\n",
    "\n",
    "Lets ask some questions of the Amazon Titan Model:\n",
    "- What is an Academy Award?\n",
    "- In what year was the first Academy Awards ceremony?\n",
    "- Who holds the record for the most Oscars won?\n",
    "- Who nominates oscar nominees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6040ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json \n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "You are a question and answer chatbot. Please answer the following question.\n",
    "\n",
    "{QUESTION}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b60e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    input_text = prompt_data.replace(\"{QUESTION}\", \"What is an Academy Award?\")\n",
    "    body = json.dumps({\n",
    "      \"inputText\": input_text, \n",
    "      \"textGenerationConfig\":{  \n",
    "        \"maxTokenCount\":128,\n",
    "        \"stopSequences\":[\"User:\"], #define phrases that signal the model to conclude text generation.\n",
    "        \"temperature\":0, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "        \"topP\":0.9 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "      }\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"amazon.titan-text-express-v1\",\n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "    print(outputText)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a9709",
   "metadata": {},
   "source": [
    "Change the prompt_data value above to try different questions.\n",
    "\n",
    "Here are some examples that should work:\n",
    "- What is an Academy Award?\n",
    "- In what year was the first Academy Awards ceremony?\n",
    "- Who holds the record for the most Oscars won?\n",
    "- Who nominates oscar nominees?\n",
    "\n",
    "So far so good. Now let's try a few question where the model may not answer correctly:\n",
    "- What are the “Big Five” awards?\n",
    "- In what year did ‘Forrest Gump’ win Best Picture?\n",
    "- In what year was the first Oscar for Best Animated Feature awarded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ab513-a369-405a-80e4-226aea301800",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    input_text = prompt_data.replace(\"{QUESTION}\", \"What are the “Big Five” awards?\")\n",
    "    body = json.dumps({\n",
    "      \"inputText\": input_text, \n",
    "      \"textGenerationConfig\":{  \n",
    "        \"maxTokenCount\":128,\n",
    "        \"stopSequences\":[\"User:\"], #define phrases that signal the model to conclude text generation.\n",
    "        \"temperature\":0, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "        \"topP\":0.9 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "      }\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"amazon.titan-text-express-v1\",\n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "    print(outputText)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a901974-a960-4e51-9a3b-cab4428f881f",
   "metadata": {},
   "source": [
    "The model hallucinated, so let's try to fix it with prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "You are a question and answer chatbot. Please answer the following question. Say \"I don't know\" if you are not sure.\n",
    "\n",
    "{QUESTION}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234844fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_text = prompt_data.replace(\"{QUESTION}\", \"What are the “Big Five” awards?\")\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"inputText\": input_text, \n",
    "      \"textGenerationConfig\":{  \n",
    "        \"maxTokenCount\":128,\n",
    "        \"stopSequences\":[\"User:\"], #define phrases that signal the model to conclude text generation.\n",
    "        \"temperature\":0, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "        \"topP\":0.9 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "      }\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"amazon.titan-text-express-v1\",\n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "    print(outputText)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4564438",
   "metadata": {},
   "source": [
    "## In-Context Learning\n",
    "Now let's help the model answer this correctly.\n",
    "\n",
    "Following passage is copied from Wikipedia: [List of Big Five Academy Award winners and nominees.](https://en.wikipedia.org/wiki/List_of_Big_Five_Academy_Award_winners_and_nominees) Let's also feed this passage to Titan as context.\n",
    "\n",
    ">At the Academy Awards, the so-called \"Big Five\" awards are those for Best Picture, Best Director, Best Actor, Best Actress, and Best Screenplay (either Best Original Screenplay or Best Adapted Screenplay).[1] As of the 94th Academy Awards (2021), a total of 43 films have been nominated in all five of these award categories. Only three films have won all five of these major awards: It Happened One Night (1934), One Flew Over the Cuckoo's Nest (1975), and The Silence of the Lambs (1991). Eight films failed to win any of the five major awards after being nominated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a71f9-aba9-44e3-bff9-786e9ba4068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "At the Academy Awards, the so-called \"Big Five\" awards are those for Best Picture, \n",
    "Best Director, Best Actor, Best Actress, and Best Screenplay (either Best Original \n",
    "Screenplay or Best Adapted Screenplay).[1] As of the 94th Academy Awards (2021), a \n",
    "total of 43 films have been nominated in all five of these award categories. Only \n",
    "three films have won all five of these major awards: It Happened One Night (1934), \n",
    "One Flew Over the Cuckoo's Nest (1975), and The Silence of the Lambs (1991). Eight \n",
    "films failed to win any of the five major awards after being nominated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "You are a question and answer chatbot. Please answer the following question only use the context below. Say \"I don't know\" if you are not sure.\n",
    "\n",
    "{CONTEXT}\n",
    "\n",
    "{QUESTION}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe682a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    input_text = prompt_data.replace(\"{QUESTION}\", \"What are the “Big Five” awards?\")\n",
    "    input_text = input_text.replace(\"{CONTEXT}\", context)\n",
    "\n",
    "\n",
    "    body = json.dumps({\n",
    "      \"inputText\": input_text, \n",
    "      \"textGenerationConfig\":{  \n",
    "        \"maxTokenCount\":128,\n",
    "        \"stopSequences\":[], #define phrases that signal the model to conclude text generation.\n",
    "        \"temperature\":0, #Temperature controls randomness; higher values increase diversity, lower values boost predictability.\n",
    "        \"topP\":0.9 # Top P is a text generation technique, sampling from the most probable tokens in a distribution.\n",
    "      }\n",
    "    })\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "      body=body,\n",
    "\t  modelId=\"amazon.titan-text-express-v1\",\n",
    "      accept=\"application/json\", \n",
    "      contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "    print(outputText)\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46449016",
   "metadata": {},
   "source": [
    ">Thank you, you have completed this section of the workshop."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
