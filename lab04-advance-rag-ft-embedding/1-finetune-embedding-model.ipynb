{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8595e0d2-e42c-49ad-aec9-bdbd7a3a802f",
   "metadata": {},
   "source": [
    "## Fine-tune Embedding Models\n",
    "\n",
    "Fine-tuning the embedding model is a critical step in enhancing the performance of RAG systems. These systems rely on retrieving relevant information from a corpus to augment the language model's generation capabilities. However, pre-trained embedding models are often trained on general-purpose datasets, which may not accurately capture the nuances and semantics specific to a particular domain or use case. Fine-tuning the embedding model on domain-specific data allows the RAG system to adapt to the target domain, improving the relevance and accuracy of retrieved information. \n",
    "\n",
    "In this notebook, we will fine-tune an open-source sentence transformers embedding model using Amazon SageMaker. Hugging Face Sentence Transformers is a Python framework for generating high-quality sentence, text, and image embeddings using state-of-the-art models. The example model we will use is `sentence-transformers/msmarco-bert-base-dot-v5`. The same technique applies to all other sentence transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52691b2-3af6-4fcd-b339-5d1b5e55adf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq sagemaker\n",
    "!pip install -Uq sentence-transformers\n",
    "!pip install -Uq PyPDF2==3.0.1\n",
    "!pip install -Uq langchain==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcda3f-92c9-4974-8e1d-ac3ca8e82a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "prefix = \"finetune-embedding\"\n",
    "model_id = \"sentence-transformers/msmarco-bert-base-dot-v5\"\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80536eb-9ce7-42df-bf16-73457b79df94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = \"data/train_dataset.json\"\n",
    "\n",
    "train_s3_path = f\"s3://{bucket}/{prefix}/{train_data}\"\n",
    "\n",
    "!aws s3 cp {train_data} {train_s3_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf7008-70c2-41ec-8694-f78f823f477d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_data = \"data/val_dataset.json\"\n",
    "\n",
    "valid_s3_path = f\"s3://{bucket}/{prefix}/{valid_data}\"\n",
    "\n",
    "!aws s3 cp {valid_data} {valid_s3_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1782b96-41f6-40a7-9143-d42f23d718ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-sentence-transformer-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,                             # pre-trained model\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 10,\n",
    "    \"evaluation_steps\": 50\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = \"train.py\",      # train script\n",
    "    source_dir           = \"scripts\",         # directory which includes all the files needed for training\n",
    "    instance_type        = \"ml.p3.2xlarge\",   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 100,               # the size of the EBS volume in GB\n",
    "    transformers_version = \"4.28\",            # the transformers version used in the training job\n",
    "    pytorch_version      = \"2.0\",             # the pytorch_version version used in the training job\n",
    "    py_version           = \"py310\",           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33438d-0b8c-4951-a806-7a096b6269c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"train\": train_s3_path, \"valid\": valid_s3_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3a246-ec62-4869-9dcd-0c3b35621014",
   "metadata": {},
   "source": [
    "### > Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6539034-b113-4028-b9de-60f9b8063349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = huggingface_estimator.model_data.split('/')[-1]\n",
    "\n",
    "!aws s3 cp {huggingface_estimator.model_data} {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b040dac-06c3-4d9d-93f8-31300ebeb464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf model && mkdir model\n",
    "!tar -xzf {filename} -C model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf9258-0895-45c6-8de9-aacfcf7e2990",
   "metadata": {},
   "source": [
    "## Evaluate Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cc4b6-a6cf-427e-881e-9c82d9cd6f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(train_data, 'r+') as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "with open(valid_data, 'r+') as f:\n",
    "    val_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba97240-91fb-4563-8807-5b6c6a3671ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_top_hit(dataset, embeddings, top_k=5):\n",
    "    \n",
    "    corpus = dataset['corpus']\n",
    "    queries = dataset['queries']\n",
    "    relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "    docs = [Document(metadata=dict(id_=id_), page_content=text) for id_, text in corpus.items()] \n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    eval_results = []\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        retrieved_docs = db.similarity_search(query, top_k)\n",
    "        retrieved_ids = [doc.metadata['id_'] for doc in retrieved_docs]\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "        eval_result = {\n",
    "            'is_hit': is_hit,\n",
    "            'retrieved': retrieved_ids,\n",
    "            'expected': expected_id,\n",
    "            'query': query_id,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdaa6e5-f9f6-4bc2-9e4f-a9becdee80ec",
   "metadata": {},
   "source": [
    "### > Evaluate percentage of top hit with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473767d8-deac-49de-b331-d61fd6ed7b3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=model_id)\n",
    "\n",
    "eval_results = evaluate_top_hit(val_dataset, base_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f503f-054a-4fa4-83aa-623872ea70de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame(eval_results)\n",
    "top_hits = df_base['is_hit'].mean()\n",
    "\n",
    "print(\"percent of top hits: {:.2f} %\".format(top_hits*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06812c4c-e3a3-480f-b183-05c35f2c1d4c",
   "metadata": {},
   "source": [
    "### > Evaluate topic hit for fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cbc8c-86ff-4e38-bc43-c0a81dcbfa6e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "finetuned_embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "\n",
    "eval_results = evaluate_top_hit(val_dataset, finetuned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96657e-2018-4f85-b2ba-edcaca4b23eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_finetuned = pd.DataFrame(eval_results)\n",
    "top_hits = df_finetuned['is_hit'].mean()\n",
    "\n",
    "print(\"percent of top hits: {:.2f} %\".format(top_hits*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ed850-d8ea-44c9-b4dd-337ffe3fced8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_base['model'] = 'base'\n",
    "df_finetuned['model'] = 'fine_tuned'\n",
    "df_all = pd.concat([df_base, df_finetuned])\n",
    "df_all.groupby('model').mean('is_hit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cecf76-0673-44d5-9333-ebf144b5368e",
   "metadata": {},
   "source": [
    "## Evaluate using `InformationRetrievalEvaluator` from sentence_transformers.\n",
    "\n",
    "This provides a more comprehensive set of embeeding metrics for sentencetransformers compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a60b2-7f47-4f5a-b389-35497eb647b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def evaluate_sentence_transformers(\n",
    "    dataset,\n",
    "    model,\n",
    "    name,\n",
    "):\n",
    "    corpus = dataset['corpus']\n",
    "    queries = dataset['queries']\n",
    "    relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)\n",
    "    return evaluator(model, output_path='results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66684b8-8aac-4029-be2b-2feddde6aa9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = SentenceTransformer(model_id, device=\"cuda\")\n",
    "finetuned_model = SentenceTransformer(model_path, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63657c4-1110-4dc5-847f-6f49cfefdcc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf results && mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afada7-a3bb-4233-a004-d6c5da11cd90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_sentence_transformers(val_dataset, base_model, name='base')\n",
    "evaluate_sentence_transformers(val_dataset, finetuned_model, name='finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377631b-8899-42d4-ba19-34a8afa9f680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_st_base = pd.read_csv('results/Information-Retrieval_evaluation_base_results.csv')\n",
    "df_st_finetuned = pd.read_csv('results/Information-Retrieval_evaluation_finetuned_results.csv')\n",
    "\n",
    "df_st_base['model'] = 'base'\n",
    "df_st_finetuned['model'] = 'fine_tuned'\n",
    "df_st = pd.concat([df_st_base, df_st_finetuned])\n",
    "df_st = df_st.set_index('model')\n",
    "df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332fbe1-a779-4688-8081-b8252a966d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store train_s3_path\n",
    "%store valid_s3_path\n",
    "%store prefix\n",
    "%store model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84237e-a94c-4e68-8ef7-c8fdbcc376c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
